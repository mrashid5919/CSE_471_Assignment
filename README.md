## Group Info

**Member 1 : 1905099 - Alina Zaman**  
**Member 2 : 1905103 - Mayesha Rashid**  
**Member 3 : 1905119 - Saha Kuljit Shantanu**

**Group ID: 6**

### <a href="https://abir66.hashnode.dev/llm-evaluators-recognize-and-favor-their-own-generations">LLM Evaluators Recognize and Favor Their Own Generations</a>

This blog provides a concise overview of the paper "[LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2404.13076)" by Arjun Panickssery, Samuel R. Bowman, and Shi Feng. It introduces the concepts of self-preference and self-recognition in large language models (LLMs), summarizes the paper's main findings and experimental approaches.

This blog defines self-preference as an LLM's tendency to rate its own outputs higher than those of other LLMs or humans, and self-recognition as an LLM's ability to distinguish its outputs from others. It highlights the paper's key discoveries:
  1. Frontier LLMs exhibit self-preference.
  2. LLMs possess non-trivial self-recognition capabilities out of the box.
  3. Fine-tuning enhances self-recognition accuracy, with models achieving over 90% accuracy after training on 500 examples.
  4. There is a linear correlation between self-recognition and self-preference strength.

The blog demonstrates complex research into an accessible format, making it understandable for readers with a basic understanding of machine learning concepts. The content reflects recent developments in AI research. It follows a logical progression, moving from definitions to experimental results. Transitions between sections are smooth, aiding readability. The use of bullet points, clear definitions, charts and graphs aids in comprehension.

## Conclusion

This blog post serves as a informative summary of the referenced paper, highlighting important findings about biases in LLM self-evaluation.

